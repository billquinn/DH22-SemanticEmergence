{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f66c54",
   "metadata": {},
   "source": [
    "# Build word2vec Models\n",
    "\n",
    "Much of this code has been copied and adapted from [Laura Nelson's \"measuring_intersectionality\" GitHub](https://github.com/lknelson/measuring_intersectionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2472bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, string, warnings, glob, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from random import choices\n",
    "\n",
    "# Import NLTK collocation packages.\n",
    "from nltk.collocations import *\n",
    "from nltk import bigrams, word_tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ignore warnings.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Declare directory.\n",
    "abs_dir = \"/Users/williamquinn/Documents/DH/Python/MJP/\"\n",
    "\n",
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    punct = string.punctuation + '“' + '”' + '‘' + \"’\"\n",
    "    \n",
    "    lower_case = text.lower()\n",
    "    lower_case = lower_case.replace('—', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in lower_case if char not in punct])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fada9",
   "metadata": {},
   "source": [
    "## Import & Normalize Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb97cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7656, 6)\n",
      "CPU times: user 3min 22s, sys: 2.4 s, total: 3min 25s\n",
      "Wall time: 3min 26s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_mjp_id</th>\n",
       "      <th>meta_magazine</th>\n",
       "      <th>meta_type</th>\n",
       "      <th>meta_text</th>\n",
       "      <th>meta_date</th>\n",
       "      <th>meta_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Little Review</td>\n",
       "      <td>front</td>\n",
       "      <td>literatur drama music art margaret c anderson ...</td>\n",
       "      <td>1914-12-01</td>\n",
       "      <td>1914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Little Review</td>\n",
       "      <td>advertisements</td>\n",
       "      <td>for the holiday vaudevill by carolin caffin an...</td>\n",
       "      <td>1914-12-01</td>\n",
       "      <td>1914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Little Review</td>\n",
       "      <td>poetry</td>\n",
       "      <td>vol i decemb no poem richard aldington on a mo...</td>\n",
       "      <td>1914-12-01</td>\n",
       "      <td>1914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Little Review</td>\n",
       "      <td>articles</td>\n",
       "      <td>a great pilgrimpagan georg soul shakespear in ...</td>\n",
       "      <td>1914-12-01</td>\n",
       "      <td>1914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Little Review</td>\n",
       "      <td>poetry</td>\n",
       "      <td>suffici helen hoyt i wish no guardian angel i ...</td>\n",
       "      <td>1914-12-01</td>\n",
       "      <td>1914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   meta_mjp_id      meta_magazine       meta_type  \\\n",
       "0            1  The Little Review           front   \n",
       "1            2  The Little Review  advertisements   \n",
       "2            3  The Little Review          poetry   \n",
       "3            4  The Little Review        articles   \n",
       "4            5  The Little Review          poetry   \n",
       "\n",
       "                                           meta_text  meta_date  meta_year  \n",
       "0  literatur drama music art margaret c anderson ... 1914-12-01       1914  \n",
       "1  for the holiday vaudevill by carolin caffin an... 1914-12-01       1914  \n",
       "2  vol i decemb no poem richard aldington on a mo... 1914-12-01       1914  \n",
       "3  a great pilgrimpagan georg soul shakespear in ... 1914-12-01       1914  \n",
       "4  suffici helen hoyt i wish no guardian angel i ... 1914-12-01       1914  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(abs_dir + 'Output/mjp_documents.txt', sep='\\t') \\\n",
    "    .dropna()\n",
    "\n",
    "# df = df.sample(frac = 0.02)\n",
    "\n",
    "# Lower text field.\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Remove numbers & underscores from text field, which signal front matter and ads too strongly.\n",
    "df['text'] = df['text'].str.replace('\\d*_*', '')\n",
    "\n",
    "# Remove magazine titles from text fields.\n",
    "mag_titles = [t for t in df['magazine'].unique()]\n",
    "mag_titles = mag_titles + ['the freewoman', 'the new freewoman', 'the egoist']\n",
    "\n",
    "df['text'] = df['text'].replace(r'|'.join(mag_titles),\n",
    "                                ' ', regex = True)\n",
    "\n",
    "# Split text string into list of words.\n",
    "df['text'] = df['text'].str.split()\n",
    "\n",
    "# Lemmatizing reduces to a root synonym\n",
    "# Stemming: need not be a dictionary word, removes prefix and affix based on few rules.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def lemma_and_stem(list_of_words):\n",
    "    return [stemmer.stem(lemmatizer.lemmatize(w)) for w in list_of_words]\n",
    "  \n",
    "df['text'] = df['text'].apply(lemma_and_stem)\n",
    "\n",
    "# Join word list into single string.\n",
    "df['text'] = df['text'].str.join(' ')\n",
    "\n",
    "# Remove duplicate rows.\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Convert 'date' column to date time.\n",
    "df['date'] = pd.to_datetime(df['date'], format = '%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "# Create \"year\" column.\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Drop rows with na (rows that failed to convert to datetime or do not have kwic).\n",
    "df = df.dropna()\n",
    "\n",
    "# Re-name columns.\n",
    "df = df.add_prefix('meta_')\n",
    "\n",
    "# Change capitalization of 'magazine' column.\n",
    "df['meta_magazine'] = df['meta_magazine'].str.title()\n",
    "\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527c907",
   "metadata": {},
   "source": [
    "## Create List of Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43fdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 49s, sys: 4.83 s, total: 19min 54s\n",
      "Wall time: 6min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences = [sentence for text in df['meta_text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "model = gensim.models.Word2Vec(words_by_sentence, vector_size = 150, \n",
    "                               window = 15, min_count = 10, \n",
    "                               sg = 1, alpha = 0.025,\n",
    "                               batch_words = 10000, workers = 3)\n",
    "\n",
    "model.wv.save_word2vec_format(abs_dir + 'Word-Doc_Vectors/Models/mjp_w2v.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc0782",
   "metadata": {},
   "source": [
    "## Build Multiple Models for Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c12e0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "CPU times: user 13h 3min 58s, sys: 2min 33s, total: 13h 6min 32s\n",
      "Wall time: 4h 27min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#create 40 random models for constructing confidence intervals\n",
    "def gen_model(words_by_sentence, num):\n",
    "    \"\"\"\n",
    "    Takes a list of words by senence as input and a number (for naming the file)\n",
    "    Saves a word2vec model in the word2vec_robust folder\n",
    "    \"\"\"\n",
    "\n",
    "    model = gensim.models.Word2Vec(words_by_sentence, vector_size = 150, \n",
    "                                   window = 15, min_count = 10, \n",
    "                                   sg = 1, alpha = 0.025,\n",
    "                                   batch_words = 10000, workers = 3)  \n",
    "    \n",
    "    model.wv.save_word2vec_format(abs_dir + 'Word-Doc_Vectors/Models/model_%d.txt' % num)\n",
    "    \n",
    "#Number of sentences, for use in creating random sentences\n",
    "num_sent = len(words_by_sentence)\n",
    "\n",
    "for num in range(0, 40):\n",
    "    print(num)\n",
    "    \n",
    "    #extract random sample of sentences with replacement, \n",
    "    #equal to total number of sentences in the full corpus\n",
    "    gen_model(choices(words_by_sentence, k = num_sent), num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
